{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbJvxZgoTOOp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Dataset for AND gate\n",
        "X = tf.constant([[0., 0.],\n",
        "                 [0., 1.],\n",
        "                 [1., 0.],\n",
        "                 [1., 1.]], dtype=tf.float32)\n",
        "Y = tf.constant([[0.],\n",
        "                 [0.],\n",
        "                 [0.],\n",
        "                 [1.]], dtype=tf.float32)\n",
        "\n",
        "# Network parameters\n",
        "n_input = 2   # inputs\n",
        "n_hidden = 4  # hidden neurons\n",
        "n_output = 1  # output\n",
        "\n",
        "# Initialize weights and biases\n",
        "W1 = tf.Variable(tf.random.normal([n_input, n_hidden]))\n",
        "b1 = tf.Variable(tf.zeros([n_hidden]))\n",
        "W2 = tf.Variable(tf.random.normal([n_hidden, n_output]))\n",
        "b2 = tf.Variable(tf.zeros([n_output]))\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Training loop\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "        hidden = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "        output = tf.nn.sigmoid(tf.matmul(hidden, W2) + b2)\n",
        "\n",
        "        # Binary cross-entropy loss\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.keras.losses.binary_crossentropy(Y, output)\n",
        "        )\n",
        "\n",
        "    # Compute gradients\n",
        "    grads = tape.gradient(loss, [W1, b1, W2, b2])\n",
        "\n",
        "    # Update weights\n",
        "    W1.assign_sub(lr * grads[0])\n",
        "    b1.assign_sub(lr * grads[1])\n",
        "    W2.assign_sub(lr * grads[2])\n",
        "    b2.assign_sub(lr * grads[3])\n",
        "\n",
        "    # Print progress with updated weights\n",
        "    print(f\"\\nEpoch {epoch}, Loss: {loss.numpy()}\")\n",
        "    print(\"Updated W1:\\n\", W1.numpy())\n",
        "    print(\"Updated b1:\\n\", b1.numpy())\n",
        "    print(\"Updated W2:\\n\", W2.numpy())\n",
        "    print(\"Updated b2:\\n\", b2.numpy())\n",
        "\n",
        "# Final predictions\n",
        "preds = tf.round(output)\n",
        "print(\"\\nPredictions for AND gate:\")\n",
        "for inp, pred in zip(X.numpy(), preds.numpy()):\n",
        "    print(f\"{inp} -> {int(pred[0])}\")\n"
      ]
    }
  ]
}